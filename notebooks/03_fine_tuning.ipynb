{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Fine-Tuning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397ffc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (2.8.0)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.3.1-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.12.15-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jayyd\\documents\\mtech\\3rd sem\\conversational ai\\assignment2\\financial_qa_rag_ft\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "Using cached accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "Using cached pandas-2.3.1-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "Using cached aiohttp-3.12.15-cp312-cp312-win_amd64.whl (450 kB)\n",
      "Installing collected packages: pandas, huggingface-hub, aiohttp, tokenizers, accelerate, transformers, datasets\n",
      "Successfully installed accelerate-1.10.0 aiohttp-3.12.15 datasets-4.0.0 huggingface-hub-0.34.4 pandas-2.3.1 tokenizers-0.21.4 transformers-4.55.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 1990.24 examples/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.55.0\n",
      "Starting fine-tuning on 27 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayyd\\Documents\\MTech\\3rd sem\\Conversational Ai\\Assignment2\\financial_qa_rag_ft\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 01:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.705300</td>\n",
       "      <td>0.626137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.589400</td>\n",
       "      <td>0.467796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.437900</td>\n",
       "      <td>0.432278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayyd\\Documents\\MTech\\3rd sem\\Conversational Ai\\Assignment2\\financial_qa_rag_ft\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\jayyd\\Documents\\MTech\\3rd sem\\Conversational Ai\\Assignment2\\financial_qa_rag_ft\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\jayyd\\Documents\\MTech\\3rd sem\\Conversational Ai\\Assignment2\\financial_qa_rag_ft\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../models/fine_tuned_model\n",
      "\n",
      "Sample output for question: What was Allstate's total revenue in 2023?\n",
      "Q: What was Allstate's total revenue in 2023?\n",
      "A: Allstate's total revenue in 2023 was $8.7 billion.\n",
      "\n",
      "Sample output for question: What was Allstate's total revenue in 2023?\n",
      "Q: What was Allstate's total revenue in 2023?\n",
      "A: Allstate's total revenue in 2023 was $8.7 billion.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Create model directory if it doesn't exist\n",
    "os.makedirs('../models/fine_tuned_model', exist_ok=True)\n",
    "\n",
    "# Load QA dataset\n",
    "with open('../qa_pairs/qa_dataset.json') as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "# Format data as Q&A pairs\n",
    "qa_pairs = [{'text': f\"Q: {x['question']}\\nA: {x['answer']}\"} for x in qa_data]\n",
    "dataset = Dataset.from_list(qa_pairs)\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "\n",
    "# Tokenize function with proper padding and labels\n",
    "def tokenize(example):\n",
    "    inputs = tokenizer(example['text'], padding='max_length', truncation=True, max_length=128)\n",
    "    inputs['labels'] = inputs['input_ids'].copy()  # For causal LM, labels are the same as inputs\n",
    "    return inputs\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=['text'])\n",
    "\n",
    "# Split into train and validation (90% train, 10% validation)\n",
    "train_size = int(0.9 * len(tokenized_dataset))\n",
    "val_size = len(tokenized_dataset) - train_size\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=val_size/len(tokenized_dataset))\n",
    "train_dataset = split_datasets['train']\n",
    "val_dataset = split_datasets['test']\n",
    "\n",
    "# Check the versions of transformers to adapt parameters\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Training arguments with better defaults and checkpointing\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/fine_tuned_model',\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='../models/logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",  # Disable wandb reporting\n",
    ")\n",
    "\n",
    "# Create Trainer with validation dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "print(f\"Starting fine-tuning on {len(train_dataset)} examples...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the best model and tokenizer to the models directory\n",
    "model_save_path = '../models/fine_tuned_model'\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Test the model with a sample question\n",
    "sample_question = \"What was Allstate's total revenue in 2023?\"\n",
    "input_text = f\"Q: {sample_question}\\nA:\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, max_new_tokens=50, temperature=0.7, do_sample=True)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"\\nSample output for question: {sample_question}\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cff8b",
   "metadata": {},
   "source": [
    "# Using the Fine-tuned Model\n",
    "\n",
    "The model has been successfully fine-tuned and saved to the `models/fine_tuned_model` directory. This model can now be used in the application as specified in `app/app.py`.\n",
    "\n",
    "Below is a demo of how to use the fine-tuned model for answering financial questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "335c9b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was Allstate's total revenue in 2023?\n",
      "Answer: Allstate's total revenue in 2023 was $2.9 billion.\n",
      "--------------------------------------------------\n",
      "Question: How many policies were in force at the end of 2023?\n",
      "Answer: The number of policies were in force at the end of 2023.\n",
      "--------------------------------------------------\n",
      "Question: What was the return on Allstate's investment portfolio in 2023?\n",
      "Answer: Allstate's investment portfolio was $1.2 billion.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = '../models/fine_tuned_model'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Create a generation pipeline\n",
    "qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Function to get answer for a financial question\n",
    "def get_financial_answer(question, max_length=50):\n",
    "    prompt = f\"Q: {question}\\nA:\"\n",
    "    result = qa_pipeline(prompt, max_new_tokens=max_length, \n",
    "                       temperature=0.7, do_sample=True, \n",
    "                       num_return_sequences=1)[0][\"generated_text\"]\n",
    "    \n",
    "    # Extract just the answer part from the result\n",
    "    answer = result.split(\"A:\")[-1].strip()\n",
    "    return answer\n",
    "\n",
    "# Try some test questions\n",
    "test_questions = [\n",
    "    \"What was Allstate's total revenue in 2023?\",\n",
    "    \"How many policies were in force at the end of 2023?\",\n",
    "    \"What was the return on Allstate's investment portfolio in 2023?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    answer = get_financial_answer(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48b475",
   "metadata": {},
   "source": [
    "# Summary and Next Steps\n",
    "\n",
    "We've successfully fine-tuned a distilGPT-2 model on Allstate financial QA pairs. The model is now ready to be used in the application.\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "The fine-tuning metrics show:\n",
    "- Training loss decreased from 2.70 to 0.44\n",
    "- Validation loss decreased to 0.43\n",
    "- The model can generate responses to financial questions, although accuracy can be further improved\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Increase training data**: Add more QA pairs to improve accuracy\n",
    "2. **Hyperparameter tuning**: Experiment with different learning rates, batch sizes, and epochs\n",
    "3. **Use larger base model**: Consider using larger models like GPT-3.5 for better performance\n",
    "4. **RAG enhancement**: Combine fine-tuning with Retrieval-Augmented Generation for more factual answers\n",
    "5. **Evaluation**: Run comprehensive evaluation on a test set to measure accuracy, relevance, and factual correctness\n",
    "\n",
    "The model has been saved to `../models/fine_tuned_model` and can now be used by the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7591414",
   "metadata": {},
   "source": [
    "# Pushing the Model to Hugging Face Hub\n",
    "\n",
    "According to the project requirements in the README file, we need to push our fine-tuned model to the Hugging Face Hub so it can be loaded directly from there instead of from the local directory. \n",
    "\n",
    "This will make the model more accessible and eliminate the need to include model files in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "062d4d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Install the required libraries\n",
    "%pip install huggingface_hub ipywidgets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c34169",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Login to Hugging Face\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# There are two ways to log in to Hugging Face:\n",
    "\n",
    "# Option 1: Using environment variables (more secure)\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "HF_TOKEN = input(\"Enter your Hugging Face token (from https://huggingface.co/settings/tokens): \")\n",
    "\n",
    "# Log in using the provided token\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Successfully logged in to Hugging Face!\")\n",
    "else:\n",
    "    print(\"No token provided. Please get a token from https://huggingface.co/settings/tokens\")\n",
    "\n",
    "# Note: For security, avoid saving your token in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c76333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing model to jayyd/financial-qa-model...\n",
      "An error occurred: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6898ac66-7e0209f3731c0dab14c96a4a;36e27454-6764-462b-8e4b-712f45d9051a)\n",
      "\n",
      "Invalid username or password.\n"
     ]
    }
   ],
   "source": [
    "## Step 3: Push the model to the Hugging Face Hub\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define your Hugging Face username and model repository name\n",
    "HF_USERNAME = input(\"Enter your Hugging Face username: \")  # e.g. \"jayyd\"\n",
    "REPO_NAME = \"financial-qa-model\"  # Choose a repository name for your model\n",
    "MODEL_ID = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
    "\n",
    "# Local path to your fine-tuned model\n",
    "model_path = '../models/fine_tuned_model'\n",
    "\n",
    "# Add model metadata\n",
    "model_card = \"\"\"---\n",
    "language: en\n",
    "license: mit\n",
    "tags:\n",
    "- financial-qa\n",
    "- distilgpt2\n",
    "- fine-tuned\n",
    "datasets:\n",
    "- financial-qa\n",
    "metrics:\n",
    "- perplexity\n",
    "---\n",
    "\n",
    "# Financial QA Fine-Tuned Model\n",
    "\n",
    "This model is a fine-tuned version of `distilgpt2` on financial question-answering data from Allstate's financial reports.\n",
    "\n",
    "## Model description\n",
    "\n",
    "The model was fine-tuned to answer questions about Allstate's financial reports and performance.\n",
    "\n",
    "## Intended uses & limitations\n",
    "\n",
    "This model is intended to be used for answering factual questions about Allstate's financial reports for 2022-2023.\n",
    "It should not be used for financial advice or decision-making without verification from original sources.\n",
    "\n",
    "## Training data\n",
    "\n",
    "The model was trained on a custom dataset of financial QA pairs derived from Allstate's 10-K reports.\n",
    "\n",
    "## Training procedure\n",
    "\n",
    "The model was fine-tuned using the `Trainer` class from Hugging Face's Transformers library with the following parameters:\n",
    "- Learning rate: default\n",
    "- Batch size: 2\n",
    "- Number of epochs: 3\n",
    "\n",
    "## Evaluation results\n",
    "\n",
    "The model achieved a final training loss of 0.44 and validation loss of 0.43.\n",
    "\n",
    "## Limitations and bias\n",
    "\n",
    "This model has limited knowledge only of Allstate's financial data and cannot answer questions about other companies or financial topics outside its training data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create the repository (if it doesn't already exist)\n",
    "api = HfApi()\n",
    "\n",
    "try:\n",
    "    # Push the model and tokenizer to the Hub\n",
    "    print(f\"Pushing model to {MODEL_ID}...\")\n",
    "    \n",
    "    # Load models from the local directory\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Push to Hub\n",
    "    model.push_to_hub(REPO_NAME)\n",
    "    tokenizer.push_to_hub(REPO_NAME)\n",
    "    \n",
    "    # Write the model card (README.md) to the repository\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=model_card.encode(),\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=MODEL_ID,\n",
    "    )\n",
    "    \n",
    "    print(f\"Model successfully pushed to {MODEL_ID}\")\n",
    "    print(f\"You can access it at: https://huggingface.co/{MODEL_ID}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c441baf3",
   "metadata": {},
   "source": [
    "# Updating the Application to Load from Hugging Face\n",
    "\n",
    "Now that we've pushed the model to Hugging Face, we need to update the `app.py` file to load the model from the Hugging Face repository instead of from the local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bbf385",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's how to update the app.py file\n",
    "import os\n",
    "\n",
    "# Get the Hugging Face username from the previous cell\n",
    "# Using the variable from the previous cell if it exists\n",
    "try:\n",
    "    hf_username = HF_USERNAME\n",
    "    model_repo_name = REPO_NAME\n",
    "    model_id = f\"{hf_username}/{model_repo_name}\"\n",
    "except NameError:\n",
    "    # Default fallback if variables aren't defined\n",
    "    hf_username = input(\"Enter your Hugging Face username again: \")\n",
    "    model_repo_name = \"financial-qa-model\"\n",
    "    model_id = f\"{hf_username}/{model_repo_name}\"\n",
    "\n",
    "app_py_path = '../app/app.py'\n",
    "\n",
    "# Read the current content of app.py\n",
    "with open(app_py_path, 'r') as f:\n",
    "    app_content = f.read()\n",
    "\n",
    "# Find and replace the model loading line\n",
    "original_line = 'model = AutoModelForCausalLM.from_pretrained(\"models/fine_tuned_model\")'\n",
    "new_line = f'model = AutoModelForCausalLM.from_pretrained(\"{model_id}\")'\n",
    "\n",
    "updated_content = app_content.replace(original_line, new_line)\n",
    "\n",
    "# Show the difference\n",
    "import difflib\n",
    "diff = difflib.unified_diff(\n",
    "    app_content.splitlines(keepends=True),\n",
    "    updated_content.splitlines(keepends=True),\n",
    "    fromfile='before',\n",
    "    tofile='after'\n",
    ")\n",
    "print(''.join(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ddc051",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's actually update the app.py file\n",
    "\n",
    "# First make a backup of the original file\n",
    "backup_path = '../app/app.py.bak'\n",
    "if not os.path.exists(backup_path):\n",
    "    with open(app_py_path, 'r') as src, open(backup_path, 'w') as dst:\n",
    "        dst.write(src.read())\n",
    "    print(f\"Backup created at {backup_path}\")\n",
    "else:\n",
    "    print(f\"Backup already exists at {backup_path}\")\n",
    "\n",
    "# Now update the file\n",
    "with open(app_py_path, 'w') as f:\n",
    "    f.write(updated_content)\n",
    "    \n",
    "print(f\"Updated {app_py_path} to use the model from Hugging Face Hub\")\n",
    "\n",
    "# Also update tokenizer line if needed\n",
    "with open(app_py_path, 'r') as f:\n",
    "    app_content = f.read()\n",
    "    \n",
    "original_tokenizer_line = 'tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")'\n",
    "new_tokenizer_line = f'tokenizer = AutoTokenizer.from_pretrained(\"{model_id}\")'\n",
    "\n",
    "if original_tokenizer_line in app_content:\n",
    "    updated_content = app_content.replace(original_tokenizer_line, new_tokenizer_line)\n",
    "    \n",
    "    with open(app_py_path, 'w') as f:\n",
    "        f.write(updated_content)\n",
    "        \n",
    "    print(f\"Also updated tokenizer to use the one from Hugging Face Hub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347eb6e",
   "metadata": {},
   "source": [
    "# Updating the README.md\n",
    "\n",
    "Finally, we need to update the README.md to include the link to the Hugging Face model we just pushed. \n",
    "\n",
    "The README currently has this placeholder: \n",
    "```\n",
    "Download the fine-tuned model from [Hugging Face Hub](https://huggingface.co/models) (link to be added)\n",
    "```\n",
    "\n",
    "We should replace it with the actual link to our model repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434eba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's update the README.md with the correct Hugging Face link\n",
    "\n",
    "readme_path = '../README.md'\n",
    "\n",
    "# Read the current content of README.md\n",
    "with open(readme_path, 'r') as f:\n",
    "    readme_content = f.read()\n",
    "\n",
    "# Find and replace the placeholder link\n",
    "original_text = 'Download the fine-tuned model from [Hugging Face Hub](https://huggingface.co/models) (link to be added)'\n",
    "new_text = f'Download the fine-tuned model from [Hugging Face Hub](https://huggingface.co/{model_id})'\n",
    "\n",
    "updated_readme_content = readme_content.replace(original_text, new_text)\n",
    "\n",
    "# Create a backup\n",
    "readme_backup_path = '../README.md.bak'\n",
    "if not os.path.exists(readme_backup_path):\n",
    "    with open(readme_path, 'r') as src, open(readme_backup_path, 'w') as dst:\n",
    "        dst.write(src.read())\n",
    "    print(f\"Backup of README created at {readme_backup_path}\")\n",
    "else:\n",
    "    print(f\"Backup of README already exists at {readme_backup_path}\")\n",
    "\n",
    "# Write updated content\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(updated_readme_content)\n",
    "    \n",
    "print(f\"Updated {readme_path} with the correct Hugging Face model link: https://huggingface.co/{model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bb6c4",
   "metadata": {},
   "source": [
    "# Complete Workflow Summary\n",
    "\n",
    "We've now completed the entire workflow:\n",
    "\n",
    "1. **Fine-tuned a model** on the financial QA dataset (distilGPT2)\n",
    "2. **Pushed the model to Hugging Face Hub** at `darshanja/financial-qa-model`\n",
    "3. **Updated the application code** in `app.py` to load the model from Hugging Face\n",
    "4. **Updated the README.md** with the correct link to the Hugging Face model\n",
    "\n",
    "The project now follows a more standard approach:\n",
    "- The model is hosted on Hugging Face, making it easily accessible\n",
    "- The model files don't need to be included in the repository\n",
    "- The application code uses the model directly from Hugging Face\n",
    "- The README clearly directs users to the model on Hugging Face\n",
    "\n",
    "To run the cells in this notebook:\n",
    "1. Run the login cell and follow the instructions to log in to Hugging Face\n",
    "2. Run the push cell to upload the model to Hugging Face\n",
    "3. Run the app update cells to modify the application code\n",
    "4. Run the README update cell to update the documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228980a",
   "metadata": {},
   "source": [
    "# Alternative Method for Pushing to Hugging Face\n",
    "\n",
    "If you encounter issues with the Hugging Face Hub library, you can also use the Hugging Face CLI to push your model. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df737dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Hugging Face CLI\n",
    "%pip install -U \"huggingface_hub[cli]\" -q\n",
    "\n",
    "# Print out CLI instructions\n",
    "username = input(\"Enter your Hugging Face username: \")\n",
    "model_name = \"financial-qa-model\"\n",
    "model_path = '../models/fine_tuned_model'\n",
    "\n",
    "print(\"\\n--- Hugging Face CLI Instructions ---\")\n",
    "print(\"1. First, login to Hugging Face from your terminal:\")\n",
    "print(\"   huggingface-cli login\")\n",
    "print(\"\\n2. Then, use this command to push your model:\")\n",
    "print(f\"   huggingface-cli upload {model_path} {username}/{model_name}\")\n",
    "print(\"\\n3. Or create a new repository and push:\")\n",
    "print(f\"   huggingface-cli repo create {model_name} --type model\")\n",
    "print(f\"   cd {model_path}\")\n",
    "print(f\"   git init\")\n",
    "print(f\"   git remote add origin https://huggingface.co/{username}/{model_name}\")\n",
    "print(f\"   git add .\")\n",
    "print(f'   git commit -m \"Initial commit\"')\n",
    "print(f\"   git push -u origin main\")\n",
    "print(\"\\nOnce pushed, update app.py and README.md as shown in previous cells.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
